{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai\n",
    "!pip install -qU langchain-core\n",
    "!pip install -qU tiktoken\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain-community\n",
    "!pip install -qU uvicorn\n",
    "!pip install -qU pymupdf\n",
    "!pip install -qU qdrant-client\n",
    "!pip install -qU python-dotenv\n",
    "!pip install -qU fastapi\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU fastapi\n",
    "!pip install -qU python-multipart\n",
    "!pip install -qU python-jobspy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"RefineMyResume - Agents - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from utils import *\n",
    "import os\n",
    "import getpass\n",
    "from langchain.globals import set_debug\n",
    "import chainlit as cl \n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from typing import Any, Callable, List, Optional, TypedDict, Union, Annotated\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name: str = \"gpt-3.5-turbo-0125\"\n",
    "evalutor_llm_name: str = \"gpt-4-turbo\"\n",
    "embedding_model: str = \"text-embedding-3-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmops-course/lib/python3.11/site-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! top_p is not default parameter.\n",
      "                top_p was transferred to model_kwargs.\n",
      "                Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/llmops-course/lib/python3.11/site-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! top_p is not default parameter.\n",
      "                top_p was transferred to model_kwargs.\n",
      "                Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "openai_chat_model = ChatOpenAI(temperature=0.1, top_p=0.0001, model=\"gpt-3.5-turbo\")\n",
    "openai_evalutor_model = ChatOpenAI(temperature=0.1, top_p=0.0001, model=evalutor_llm_name)\n",
    "enc = tiktoken.encoding_for_model(llm_name)\n",
    "pages = PyPDFLoader(\"data/resume.pdf\").load_and_split()\n",
    "embedding_model = OpenAIEmbeddings(model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parse_resume():\n",
    "    PROMPT_4=\"\"\"\n",
    "        You are given resume : ```{resume}```\n",
    "\n",
    "        then based on the given resume extract information about the person\n",
    "\n",
    "        {format_instructions}\n",
    "\n",
    "        \"\"\"\n",
    "    template = PROMPT_4\n",
    "    parser = PydanticOutputParser(pydantic_object=OutputFormat)\n",
    "    prompt_template_name = PromptTemplate(\n",
    "        input_variables=[\"resume\"],\n",
    "        template=template,\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "\n",
    "    name_chain = LLMChain(llm=openai_chat_model, prompt=prompt_template_name)\n",
    "    response = name_chain(inputs={\"resume\": pages})\n",
    "    resume_json=parser.parse(response[\"text\"]).json()\n",
    "    return resume_json\n",
    "    \n",
    "def scan_for_jobs():\n",
    "    jobs = scrape_jobs(\n",
    "        site_name=[\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "        search_term=\"software engineer\",\n",
    "        location=\"San Francisco, CA\",\n",
    "        results_wanted=20,\n",
    "        hours_old=72, # (only Linkedin/Indeed is hour specific, others round up to days old)\n",
    "        country_indeed='USA',  # only needed for indeed / glassdoor\n",
    "        # linkedin_fetch_description=True # get full description and direct job url for linkedin (slower)\n",
    "    )\n",
    "    print(f\"Found {len(jobs)} jobs\")\n",
    "    jobs.to_csv(\"data/jobs.csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False) # to_xlsx\n",
    "    return jobs\n",
    "    \n",
    "def write_resume_to_file(resume_json, fileName:str='data/resume.json'): \n",
    "    f=open(fileName, 'w')\n",
    "    f.write(resume_json)\n",
    "    f.close()\n",
    "        \n",
    "def get_resume(path):\n",
    "    if os.path.isfile(path):\n",
    "        with open(path) as json_file:\n",
    "            print('Loading resume.json from the disk')\n",
    "            resume_json= json.load(json_file)\n",
    "    else:\n",
    "        print('Reading pdf resume and Loading')\n",
    "        resume_json = read_parse_resume()\n",
    "        write_resume_to_file(resume_json)\n",
    "    return resume_json\n",
    "\n",
    "def get_jobs(path):\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading jobs.csv from the disk')\n",
    "        csvFile = df=pd.read_csv(path)\n",
    "    else:\n",
    "        print('Scanning web for jobs and Loading')\n",
    "        csvFile= scan_for_jobs()\n",
    "    return csvFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resume.json from the disk\n"
     ]
    }
   ],
   "source": [
    "resume = get_resume(\"data/resume.json\")\n",
    "summary = resume[\"About_Me\"][\"Summary\"]\n",
    "keywords = resume[\"Skills\"][\"Technical_Skills\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the past 20 years I have continuously challenged myself with new languages, frameworks and methodologies. A full stack web developer and have recently been working more with Angular, React/react-native, AI/ML, Android, Blockchain/Hyperledger Fabric and Composer. An expert in front-end client-side technologies, backend server-side technologies and the database technologies.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Angular',\n",
       " 'TypeScript',\n",
       " 'Python (Numpy, scikit-learn, keras, PyTorch, Pandas, TensorFlow)',\n",
       " 'RxJS',\n",
       " 'React/React-Native',\n",
       " 'Redux/Flux',\n",
       " 'Hyperledger Fabric/Composer',\n",
       " 'Blockchain',\n",
       " 'Node.js',\n",
       " 'JavaScript',\n",
       " 'IOT',\n",
       " 'Java',\n",
       " 'Android',\n",
       " 'AWS',\n",
       " 'GCP',\n",
       " 'Application Architecture',\n",
       " 'CSS3',\n",
       " 'REST',\n",
       " 'SQL',\n",
       " 'NOSQL (MongoDB, CouchDB)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading jobs.csv from the disk\n"
     ]
    }
   ],
   "source": [
    "jobs_list =get_jobs(\"data/jobs.csv\")\n",
    "firstJob = jobs_list.iloc[0]\n",
    "job_description = firstJob.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def missing_keywords():\n",
    "    \"\"\"\n",
    "        Given a set of keywords and a job description, finds all the missing keywords in the job description.\n",
    "    \"\"\"\n",
    "    SYSTEM_PROMPT = \"You are an expert in reading resume's and job description\"\n",
    "    USER_PROMPT = \"\"\"\n",
    "            You are here to find the missing keywords or skills from a Resume.             \n",
    "            \n",
    "            You are given a job description. Here is the job description:\n",
    "            {job_description}\n",
    "            \n",
    "            You are also given the list of user skillset :\n",
    "            {keywords}\n",
    "            \n",
    "            List out all the skills which are in the job description but missing from the user skillset in the order of importance.\n",
    "        \"\"\"\n",
    "    ASSISTANT_PROMPT= \"For example, GraphSQL is in the job description but missing from the resume.\"\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            messages = [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"assistant\", ASSISTANT_PROMPT),\n",
    "                (\"user\", USER_PROMPT)\n",
    "        ])\n",
    "    chain = chat_prompt | openai_chat_model ## add pydantic parser, \n",
    "    return chain.invoke({\"job_description\": job_description, \"keywords\": keywords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def inconsistencies_in_resume():\n",
    "    \"\"\"\n",
    "        Finds conflicting information, date inconsistencies in the given resume        \n",
    "    \"\"\"\n",
    "    SYSTEM_PROMPT = \"You are an expert in reading resume's\"\n",
    "    USER_PROMPT = \"\"\"\n",
    "        You are given resume ```{resume}```. \n",
    "\n",
    "        List out all the conflicting information, date inconsistencies.\n",
    "    \"\"\"\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        messages = [\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            (\"user\", USER_PROMPT)\n",
    "    ])\n",
    "    chain = chat_prompt | openai_chat_model ## add pydantic parser, \n",
    "    return chain.invoke({resume: resume})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [missing_keywords, inconsistencies_in_resume]\n",
    "llm_with_tools = openai_chat_model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_debug\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# set_debug(True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# llm_with_tools.invoke(\"Find inconsistencies in resume\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# llm_with_tools.invoke(\"Find the missing keywords in the resume\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtool_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFind the missing keywords in the resume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmops-course/lib/python3.11/site-packages/langgraph/utils.py:88\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     83\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmops-course/lib/python3.11/site-packages/langgraph/prebuilt/tool_executor.py:95\u001b[0m, in \u001b[0;36mToolExecutor._execute\u001b[0;34m(self, tool_invocation, config)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m, tool_invocation: ToolInvocationInterface, config: RunnableConfig\n\u001b[1;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtool_invocation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_map:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalid_tool_msg_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     97\u001b[0m             requested_tool_name\u001b[38;5;241m=\u001b[39mtool_invocation\u001b[38;5;241m.\u001b[39mtool,\n\u001b[1;32m     98\u001b[0m             available_tool_names_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools]),\n\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tool'"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "# set_debug(True)\n",
    "\n",
    "# llm_with_tools.invoke(\"Find inconsistencies in resume\")\n",
    "# llm_with_tools.invoke(\"Find the missing keywords in the resume\")\n",
    "tool_executor.invoke(\"Find the missing keywords in the resume\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
